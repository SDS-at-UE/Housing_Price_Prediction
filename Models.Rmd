---
title: "Models"
author: "Gideon Wolf, Jeremiah Sagers"
date: "4/15/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

### From ReadData.R
library(tidyverse)
library(mice)
library(ggplot2)
library(glmnet)
library(caret)
library(tree)
library(randomForest)

# Read in data, handle NAs
house <- read.csv("train.csv")

NAtoNone <- c(2,3,7,10,24,25,26,31,32,33,34,36,54,56,58,59,61,64,65,73,74,75,79)
for (i in NAtoNone) {
  housetmp <- house
  housetmp[i][is.na(housetmp[i])] <- "None"
  house <- housetmp
}
house <- house[-1380,]

house <- house %>% mutate_if(is.character, as.factor)
house <- house %>% mutate_at(c('MSSubClass','MoSold'), as_factor) 

YrMoSold <- do.call(paste, c(sep='',list(house$YrSold),list(rep("-", times = length(house$YrSold))),list(house$MoSold),list(rep("-01", times = length(house$YrSold)))))
YrMoSold <- as_date(YrMoSold)
house <- as.data.frame(append(house, list(YrMoSold = YrMoSold), after = 78))

# Take out GarageYrBlt
house <- house[-60]

# Imputation
numhouse <- select_if(house, is.numeric)
imputed_house <- mice(numhouse, 
                      m = 10,  
                      method = 'pmm', 
                      seed = 1)

set.seed(1)
imphouse <- complete(imputed_house)
housetmp <- house
s = 1
for (i in names(imphouse)[2:length(names(imphouse))]) {
  housetmp[i] <- imphouse[i]
}
house <- housetmp
###
```

Working on making Lasso, Ridge, and RF

```{r}
set.seed(66)
training <- mutate(house, id = row_number())
train <- sample_frac(training, .75)
test <- anti_join(training, train, by = 'id')
train <- dplyr::select(train, -id)
test <- dplyr::select(test, -id)

x_train <- model.matrix(SalePrice~.-1, train)[,-81]
y_train <- train[81]$SalePrice
x_test <- model.matrix(SalePrice~.-1, test)[,-81]
y_test <- test[81]$SalePrice
```

## Using GLMNet to find Lasso CV model
```{r}
cv <- cv.glmnet(x_train,y_train, alpha=1,nfolds = 20)
plot(cv, xlim=c(5,11))
minlam = cv$lambda.min
print("Lambda: ")
minlam
cvlas <- glmnet(x_train,y_train,family = "gaussian", alpha=1, lambda = minlam)

cvpred <- predict(cvlas, s =minlam, newx = x_test)
print("MSE: ")
mean((cvpred - y_test)^2)

# Dr. Khormali found: https://drbeane.github.io/_pages/courses/mth345/24%20-%20Lasso%20and%20Ridge.nb.html
# for cv lasso
y_train_pred_lasso <- predict(cvlas, x_train)
sse <- sum((y_train - y_train_pred_lasso)^2)
sst <- sum((y_train - mean(y_train))^2)
training_rsq <- 1 - sse / sst
y_test_pred_lasso <- cvpred
sse <- sum((y_test - y_test_pred_lasso)^2)
sst <- sum((y_test - mean(y_test))^2)
testing_rsq <- 1 - sse / sst
lasso_rsq <- c(training_rsq, testing_rsq)
names(lasso_rsq) <- c('Training Rsq', 'Testing Rsq')
lasso_rsq

# https://stats.stackexchange.com/questions/25817/is-it-possible-to-calculate-aic-and-bic-for-lasso-regression-models
fit <- cvlas
tLL <- fit$nulldev - deviance(fit)
k <- fit$df
n <- fit$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)
print("AIC: ")
AICc
```

## Using GLMNet to find Ridge Regression Model
```{r}
rid <- cv.glmnet(x_train,y_train, alpha = 0, lambda = exp(seq(from = 5,to = 15,length = 100)))
plot(rid, xlim=c(5,15))
minlam = rid$lambda.min
print("Lambda: ")
minlam
ridge <- glmnet(x_train,y_train,family = "gaussian", alpha=0, lambda = minlam)

ridpred <- predict(ridge, s =minlam, newx = x_test)
print("MSE: ")
mean((ridpred - y_test)^2)

# Dr. Khormali found: https://drbeane.github.io/_pages/courses/mth345/24%20-%20Lasso%20and%20Ridge.nb.html
# for cv lasso
y_train_pred_lasso <- predict(ridge, x_train)
sse <- sum((y_train - y_train_pred_lasso)^2)
sst <- sum((y_train - mean(y_train))^2)
training_rsq <- 1 - sse / sst
y_test_pred_lasso <- ridpred
sse <- sum((y_test - y_test_pred_lasso)^2)
sst <- sum((y_test - mean(y_test))^2)
testing_rsq <- 1 - sse / sst
lasso_rsq <- c(training_rsq, testing_rsq)
names(lasso_rsq) <- c('Training Rsq', 'Testing Rsq')
lasso_rsq

# https://stats.stackexchange.com/questions/25817/is-it-possible-to-calculate-aic-and-bic-for-lasso-regression-models
fit <- ridge
tLL <- fit$nulldev - deviance(fit)
k <- fit$df
n <- fit$nobs
AICc <- -tLL+2*k+2*k*(k+1)/(n-k-1)
print("AIC: ")
AICc
```


```{r}
rf <- randomForest(SalePrice ~ ., 
                           data = train,
                           mtry = 9,
                           ntree = 100,
                           importance = TRUE)

rfpred  <-  predict(rf, newdata = test)
print("MSE = ")
mean((y_test - rfpred)^2)
plot(rf)
```


